import numpy as np
import pandas as pd
import os
import hashlib
import json
from sklearn.preprocessing import StandardScaler

class Preprocessor:
    def __init__(self):
        """Initialize the malware detection preprocessor."""
        self.scaler = StandardScaler()
        self.feature_columns = [
            'entropy', 'file_size', 'has_signature', 'imports_count',
            'sections_count', 'suspicious_strings', 'pe_characteristics',
            'is_dll', 'is_exe', 'is_encrypted', 'has_resources',
            'has_tls', 'has_debug', 'has_exceptions', 'has_overlay'
        ]

    def preprocess(self, data):
        """Preprocess the input data for malware detection.
        
        Args:
            data: Input data, can be a dictionary with system metrics or a DataFrame
            
        Returns:
            Preprocessed data ready for the model
        """
        if isinstance(data, dict):
            # Single sample in a dictionary
            return self._preprocess_dict(data)
        
        elif isinstance(data, pd.DataFrame):
            # DataFrame with multiple samples
            return self._preprocess_dataframe(data)
        
        else:
            # Assume it's already preprocessed
            return data
    
    def _preprocess_dict(self, data):
        """Preprocess a single dictionary of metrics."""
        # Create a DataFrame with a single row
        df = pd.DataFrame([data])
        
        # Apply feature engineering
        features = self.feature_engineering(df)
        
        return features.values
    
    def _preprocess_dataframe(self, data):
        """Preprocess a DataFrame of metrics."""
        # Apply feature engineering
        features = self.feature_engineering(data)
        
        return features.values

    def preprocess_file(self, file_path):
        """Extract features from a file for malware detection.
        
        Args:
            file_path: Path to the file to analyze
            
        Returns:
            np.array: Extracted features ready for the model
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Extract basic file metrics
        file_metrics = self._extract_file_metrics(file_path)
        
        # Preprocess the metrics
        return self.preprocess(file_metrics)
    
    def _extract_file_metrics(self, file_path):
        """Extract basic metrics from a file."""
        file_size = os.path.getsize(file_path)
        
        # Calculate entropy
        entropy = self._calculate_file_entropy(file_path)
        
        # In a real implementation, you would extract more sophisticated features
        # such as PE headers, imports, sections, strings, etc.
        # For this example, we'll use some placeholder values
        
        # Create a dictionary with 35 features (to match our training data)
        metrics = {
            'entropy': entropy,
            'file_size': file_size,
            'has_signature': 0,  # Placeholder
            'imports_count': 0,  # Placeholder
            'sections_count': 0,  # Placeholder
            'suspicious_strings': 0,  # Placeholder
            'pe_characteristics': 0,  # Placeholder
            'is_dll': 0,  # Placeholder
            'is_exe': 1,  # Placeholder
            'is_encrypted': 0,  # Placeholder
            'has_resources': 0,  # Placeholder
            'has_tls': 0,  # Placeholder
            'has_debug': 0,  # Placeholder
            'has_exceptions': 0,  # Placeholder
            'has_overlay': 0   # Placeholder
        }
        
        # Add additional features to match the 35 features used in training
        for i in range(1, 21):
            metrics[f'feature_{i}'] = np.random.random()  # Generate random values for demo
            
        return metrics
    
    def _calculate_file_entropy(self, file_path):
        """Calculate Shannon entropy of a file."""
        with open(file_path, 'rb') as f:
            data = f.read()
        
        if not data:
            return 0
        
        entropy = 0
        for x in range(256):
            p_x = data.count(bytes([x])) / len(data)
            if p_x > 0:
                entropy += -p_x * np.log2(p_x)
        
        return entropy

    def feature_engineering(self, data):
        """Extract and normalize features for malware detection.
        
        Args:
            data: DataFrame with raw metrics
            
        Returns:
            DataFrame with engineered features
        """
        # Ensure all required columns exist
        for col in self.feature_columns:
            if col not in data.columns:
                data[col] = 0
        
        # Select only the relevant features
        features = data[self.feature_columns].copy()
        
        # Add derived features
        features['size_entropy_ratio'] = features['file_size'] / (features['entropy'] + 1)
        
        # Log transform file size (common for malware analysis)
        features['log_file_size'] = np.log1p(features['file_size'])
        
        return features
